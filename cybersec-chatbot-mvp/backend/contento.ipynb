{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70e6b31",
   "metadata": {},
   "source": [
    "os - Lets us work with files and folders\n",
    "\n",
    "json - Helps handle JSON data (a common data format)\n",
    "\n",
    "re - For working with text patterns (regular expressions)\n",
    "\n",
    "typing - Helps specify what types of data our functions work with\n",
    "\n",
    "Path - A better way to handle file paths\n",
    "\n",
    "openai - The official OpenAI library\n",
    "\n",
    "dotenv - Reads secret keys from a .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbc1758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os - Lets us work with files and folders\n",
    "\n",
    "# json - Helps handle JSON data (a common data format)\n",
    "\n",
    "# re - For working with text patterns (regular expressions)\n",
    "\n",
    "# typing - Helps specify what types of data our functions work with\n",
    "\n",
    "# Path - A better way to handle file paths\n",
    "\n",
    "# openai - The official OpenAI library\n",
    "\n",
    "# dotenv - Reads secret keys from a .env file\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac418e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads hidden environment variables (like API keys) from a .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6caae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Creates a new OpenAI client using our secret API key\n",
    "        # Think of this like logging into your OpenAI account\n",
    "        self.openai_client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    def process_markdown_file(self, file_path: str) -> Dict:\n",
    "        # Opens a markdown file and reads all its text\n",
    "        # 'r' means \"read mode\"\n",
    "        # 'utf-8' handles special characters correctly\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Looks for the main title (line starting with #)\n",
    "        # If no title found, uses the filename (without extension) as title\n",
    "        title_match = re.search(r'^#\\s+(.+)$', content, re.MULTILINE)\n",
    "        title = title_match.group(1) if title_match else Path(file_path).stem\n",
    "\n",
    "        # Splits content into sections (by ## headings)\n",
    "        # Extracts useful info about the content (topics, examples etc.)\n",
    "        sections = self.split_into_sections(content)\n",
    "        metadata = self.extract_metadata(content, file_path)\n",
    "\n",
    "        # Returns all the organized information in a dictionary\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"file_path\": file_path,\n",
    "            \"sections\": sections,\n",
    "            \"metadata\": metadata,\n",
    "            \"full_content\": content\n",
    "        }\n",
    "    \n",
    "\n",
    "        # Prepares empty containers to hold sections we'll find\n",
    "    def split_into_sections(self, content: str) -> List[Dict]:\n",
    "        sections = []\n",
    "        current_section = \"\"\n",
    "        current_heading = \"\"\n",
    "\n",
    "        # When we find a section heading (##):\n",
    "        # Saves the previous section (if any exists)\n",
    "        # Records the heading, content, and word count\n",
    "        lines = content.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('## '):\n",
    "                if current_section and current_heading:\n",
    "                    sections.append({\n",
    "                        \"heading\": current_heading,\n",
    "                        \"content\": current_section.strip(),\n",
    "                        \"word_count\": len(current_section.split())\n",
    "                    })\n",
    "                    current_heading = line[3:].strip()\n",
    "                    current_section = \"\"\n",
    "                else:\n",
    "                    current_section += line + '\\n'\n",
    "\n",
    "            # Makes sure the last section gets saved\n",
    "            # Returns all sections found\n",
    "            if current_section and current_heading:\n",
    "                sections.append({\n",
    "                    \"heading\": current_heading,\n",
    "                    \"content\": current_section.strip(),\n",
    "                    \"word_count\": len(current_section.split())\n",
    "                })\n",
    "            return sections\n",
    "    \n",
    "    \n",
    "    def extract_metadata(self, content: str, file_path: str) -> Dict:\n",
    "            # Sets up default metadata values\n",
    "            metadata = {\n",
    "                \"platform\": \"picoCTF\",\n",
    "                \"difficulty\": \"beginner\",\n",
    "                \"topics\": [],\n",
    "                \"examples\": []\n",
    "            }\n",
    "\n",
    "            # Prepares to search for topic keywords (all in lowercase)\n",
    "            content_lower = content.lower()\n",
    "            topic_keywords = {\n",
    "            \"web_security\": [\"sql injection\", \"xss\", \"csrf\", \"web\", \"http\", \"cookie\"],\n",
    "            \"cryptography\": [\"encryption\", \"cipher\", \"crypto\", \"hash\", \"rsa\", \"aes\"],\n",
    "            \"reverse_engineering\": [\"assembly\", \"disassembly\", \"binary\", \"executable\"],\n",
    "            \"forensics\": [\"steganography\", \"file analysis\", \"metadata\", \"recovery\"],\n",
    "            \"pwn\": [\"buffer overflow\", \"stack\", \"heap\", \"memory\", \"exploit\"],\n",
    "            \"networking\": [\"tcp\", \"udp\", \"packet\", \"wireshark\", \"network\"]\n",
    "            }\n",
    "\n",
    "            # If any keyword matches, adds that topic to metadata\n",
    "            for topic, keywords in topic_keywords.items():\n",
    "                if any(keyword in content_lower for keyword in keywords):\n",
    "                    metadata[\"topics\"].append(topic)\n",
    "\n",
    "\n",
    "            # Finds all code blocks (between triple backticks ```)\n",
    "            # Keeps max 3 examples\n",
    "            # Returns complete metadata\n",
    "            examples = re.findall(r'```[\\s\\S]*?```', content)\n",
    "            metadata[\"examples\"] = examples[:3]\n",
    "            return metadata\n",
    "\n",
    "    def create_embedding(self, text: str) -> List[float]:\n",
    "        try:\n",
    "            # Sends text to OpenAI to get its \"embedding\" (numerical representation)\n",
    "            # Limits to first 8000 characters to avoid errors\n",
    "            # Returns the embedding numbers\n",
    "            response = self.openai_client.embeddings.create(\n",
    "                model=\"text-embedding-ada-002\",\n",
    "                input=text[:8000]\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        \n",
    "        # If something fails, prints error and returns empty list\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embedding: {e}\")\n",
    "            return []\n",
    "    \n",
    "    \n",
    "    def process_ctf_primer_directory(self, primer_path: str) -> List[Dict]:\n",
    "        processed_content = []\n",
    "        # Looks through all folders and subfolders\n",
    "        # Finds all .adoc files\n",
    "        # Prints which file is being processed  \n",
    "        for root, dirs, files in os.walk(primer_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.adoc'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    print(f\"Processing: {file_path}\")\n",
    "\n",
    "                    try:\n",
    "                        # Processes each file\n",
    "                        # Creates embeddings for each section\n",
    "                        # Adds to final results list\n",
    "                        content_data = self.process_markdown_file(file_path)\n",
    "                        for section in content_data[\"sections\"]:\n",
    "                            embedding = self.create_embedding(section[\"content\"])\n",
    "                            section[\"embedding\"] = embedding\n",
    "                        processed_content.append(content_data)\n",
    "\n",
    "                        # Skips files that cause errors but keeps processing others\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {e}\")\n",
    "                        continue\n",
    "        return processed_content\n",
    "    \n",
    "    def save_processed_content(self, processed_content: List[Dict], output_path: str):\n",
    "        # Creates folder if it doesn't exist\n",
    "        # Saves data as nicely formatted JSON file\n",
    "        # Prints where it saved the file\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_content, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Processed content saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3058f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./content/raw/ctf-primer\\book.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\assembly.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\binary.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\c.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\careers.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\code.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\crypto.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\environment.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\forensics.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\git.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\intro.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\network.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\python.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\regex.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\reversing.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\shell.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\sql.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\tools.adoc\n",
      "Processing: ./content/raw/ctf-primer\\chapters\\web.adoc\n",
      "Processed content saved to: ./content/processed/ctf_primer_processed.json\n",
      "Processed 19 files\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Creates processor\n",
    "\n",
    "    # Checks if CTF content exists\n",
    "\n",
    "    # Processes all files if they exist\n",
    "\n",
    "    # Or shows helpful message if they don't\n",
    "    \n",
    "    processor = ContentProcessor()\n",
    "    ctf_primer_path = \"./content/raw/ctf-primer\"\n",
    "    \n",
    "    if os.path.exists(ctf_primer_path):\n",
    "        processed_content = processor.process_ctf_primer_directory(ctf_primer_path)\n",
    "        processor.save_processed_content(\n",
    "            processed_content, \n",
    "            \"./content/processed/ctf_primer_processed.json\"\n",
    "        )\n",
    "        print(f\"Processed {len(processed_content)} files\")\n",
    "    else:\n",
    "        print(f\"Path not found: {ctf_primer_path}\")\n",
    "        print(\"Please clone the repository first:\")\n",
    "        print(\"git clone https://github.com/picoCTF/ctf-primer.git ./content/raw/ctf-primer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
